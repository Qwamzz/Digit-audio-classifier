{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6664f90",
   "metadata": {},
   "source": [
    "# Spoken Digit Classification with Audio\n",
    "\n",
    "This notebook implements a lightweight solution for classifying spoken digits (0-9) using the Free Spoken Digit Dataset (FSDD).\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9db1917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import load_dataset\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73334f2",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration\n",
    "\n",
    "Load the FSDD dataset from Hugging Face and explore its characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "accf00c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 2700\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchcodec.decoders'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset[\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Display sample information\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m sample = \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSample keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample.keys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAudio shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample[\u001b[33m'\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33marray\u001b[39m\u001b[33m'\u001b[39m].shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NiiYarteyGidiglo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\arrow_dataset.py:2859\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2857\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33marrow\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpandas\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpolars\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2858\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m Column(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[32m-> \u001b[39m\u001b[32m2859\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NiiYarteyGidiglo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\arrow_dataset.py:2841\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2839\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m   2840\u001b[39m pa_subtable = query_table(\u001b[38;5;28mself\u001b[39m._data, key, indices=\u001b[38;5;28mself\u001b[39m._indices)\n\u001b[32m-> \u001b[39m\u001b[32m2841\u001b[39m formatted_output = \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2842\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpa_subtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_all_columns\u001b[49m\n\u001b[32m   2843\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2844\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NiiYarteyGidiglo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\formatting\\formatting.py:657\u001b[39m, in \u001b[36mformat_table\u001b[39m\u001b[34m(table, key, formatter, format_columns, output_all_columns)\u001b[39m\n\u001b[32m    655\u001b[39m python_formatter = PythonFormatter(features=formatter.features)\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m657\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NiiYarteyGidiglo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\formatting\\formatting.py:410\u001b[39m, in \u001b[36mFormatter.__call__\u001b[39m\u001b[34m(self, pa_table, query_type)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa.Table, query_type: \u001b[38;5;28mstr\u001b[39m) -> Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[32m    409\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mrow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mformat_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m query_type == \u001b[33m\"\u001b[39m\u001b[33mcolumn\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    412\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.format_column(pa_table)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NiiYarteyGidiglo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\formatting\\formatting.py:459\u001b[39m, in \u001b[36mPythonFormatter.format_row\u001b[39m\u001b[34m(self, pa_table)\u001b[39m\n\u001b[32m    457\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyRow(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    458\u001b[39m row = \u001b[38;5;28mself\u001b[39m.python_arrow_extractor().extract_row(pa_table)\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m row = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpython_features_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m row\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NiiYarteyGidiglo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\formatting\\formatting.py:223\u001b[39m, in \u001b[36mPythonFeaturesDecoder.decode_row\u001b[39m\u001b[34m(self, row)\u001b[39m\n\u001b[32m    222\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, row: \u001b[38;5;28mdict\u001b[39m) -> \u001b[38;5;28mdict\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.features \u001b[38;5;28;01melse\u001b[39;00m row\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NiiYarteyGidiglo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\features\\features.py:2093\u001b[39m, in \u001b[36mFeatures.decode_example\u001b[39m\u001b[34m(self, example, token_per_repo_id)\u001b[39m\n\u001b[32m   2078\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode_example\u001b[39m(\u001b[38;5;28mself\u001b[39m, example: \u001b[38;5;28mdict\u001b[39m, token_per_repo_id: Optional[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]]] = \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   2079\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Decode example with custom feature decoding.\u001b[39;00m\n\u001b[32m   2080\u001b[39m \n\u001b[32m   2081\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2089\u001b[39m \u001b[33;03m        `dict[str, Any]`\u001b[39;00m\n\u001b[32m   2090\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   2092\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m-> \u001b[39m\u001b[32m2093\u001b[39m         column_name: \u001b[43mdecode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2094\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._column_requires_decoding[column_name]\n\u001b[32m   2095\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m value\n\u001b[32m   2096\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m column_name, (feature, value) \u001b[38;5;129;01min\u001b[39;00m zip_dict(\n\u001b[32m   2097\u001b[39m             {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m example}, example\n\u001b[32m   2098\u001b[39m         )\n\u001b[32m   2099\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NiiYarteyGidiglo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\features\\features.py:1405\u001b[39m, in \u001b[36mdecode_nested_example\u001b[39m\u001b[34m(schema, obj, token_per_repo_id)\u001b[39m\n\u001b[32m   1402\u001b[39m \u001b[38;5;66;03m# Object with special decoding:\u001b[39;00m\n\u001b[32m   1403\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(schema, \u001b[33m\"\u001b[39m\u001b[33mdecode_example\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(schema, \u001b[33m\"\u001b[39m\u001b[33mdecode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1404\u001b[39m     \u001b[38;5;66;03m# we pass the token to read and decode files from private repositories in streaming mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1405\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_per_repo_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1406\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NiiYarteyGidiglo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\features\\audio.py:170\u001b[39m, in \u001b[36mAudio.decode_example\u001b[39m\u001b[34m(self, value, token_per_repo_id)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Decode example audio file into audio data.\u001b[39;00m\n\u001b[32m    154\u001b[39m \n\u001b[32m    155\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m \u001b[33;03m    `torchcodec.decoders.AudioDecoder`\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.TORCHCODEC_AVAILABLE:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_torchcodec\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTo support decoding audio data, please install \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtorchcodec\u001b[39m\u001b[33m'\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NiiYarteyGidiglo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\datasets\\features\\_torchcodec.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchcodec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecoders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder \u001b[38;5;28;01mas\u001b[39;00m _AudioDecoder\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mAudioDecoder\u001b[39;00m(_AudioDecoder):\n\u001b[32m      6\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchcodec.decoders'"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset(\"mteb/free-spoken-digit-dataset\")\n",
    "print(f\"Dataset size: {len(dataset['train'])}\")\n",
    "\n",
    "# Display sample information\n",
    "sample = dataset['train'][0]\n",
    "print(f\"Sample keys: {sample.keys()}\")\n",
    "print(f\"Audio shape: {sample['audio']['array'].shape}\")\n",
    "print(f\"Sample rate: {sample['audio']['sampling_rate']}Hz\")\n",
    "print(f\"Label: {sample['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e2329",
   "metadata": {},
   "source": [
    "## 2. Audio Preprocessing\n",
    "\n",
    "Create functions for audio preprocessing including resampling and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09529607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(waveform, sample_rate, target_sample_rate=8000, target_length=8000):\n",
    "    \"\"\"Preprocess audio by resampling and padding/truncating\"\"\"\n",
    "    # Resample if necessary\n",
    "    if sample_rate != target_sample_rate:\n",
    "        waveform = librosa.resample(waveform, orig_sr=sample_rate, target_sr=target_sample_rate)\n",
    "    \n",
    "    # Pad or truncate to target length\n",
    "    if len(waveform) > target_length:\n",
    "        waveform = waveform[:target_length]\n",
    "    else:\n",
    "        waveform = np.pad(waveform, (0, target_length - len(waveform)))\n",
    "    \n",
    "    # Normalize (avoid division by zero)\n",
    "    max_val = np.max(np.abs(waveform))\n",
    "    if max_val > 0:\n",
    "        waveform = waveform / max_val\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eed95cf",
   "metadata": {},
   "source": [
    "## 3. Feature Extraction\n",
    "\n",
    "Extract Mel spectrograms from the audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745b4469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(waveform, sample_rate=8000, n_mels=64, n_fft=1024, hop_length=512):\n",
    "    \"\"\"Extract mel spectrogram features from audio\"\"\"\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=waveform,\n",
    "        sr=sample_rate,\n",
    "        n_mels=n_mels,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length\n",
    "    )\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return mel_spec_db\n",
    "\n",
    "# Create custom dataset\n",
    "class SpokenDigitDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "        waveform = item['audio']['array']\n",
    "        sample_rate = item['audio']['sampling_rate']\n",
    "        \n",
    "        # Preprocess audio\n",
    "        waveform = preprocess_audio(waveform, sample_rate)\n",
    "        \n",
    "        # Extract features\n",
    "        features = extract_features(waveform)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        features = torch.FloatTensor(features).unsqueeze(0)\n",
    "        label = torch.tensor(int(item['label']))\n",
    "        \n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730faef3",
   "metadata": {},
   "source": [
    "## 4. Model Architecture\n",
    "\n",
    "Define a lightweight CNN for audio classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff38d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e9afee",
   "metadata": {},
   "source": [
    "## 5. Training Pipeline\n",
    "\n",
    "Set up the training loop with loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bcc2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "train_dataset = SpokenDigitDataset(dataset['train'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Initialize model, loss, and optimizer\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AudioCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for features, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        features, labels = features.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    epoch_acc = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b00e448",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "\n",
    "Evaluate model performance using confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5246c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, dataset):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for features, labels in DataLoader(dataset, batch_size=32):\n",
    "            features = features.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(features)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print('\\nClassification Report:')\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "\n",
    "evaluate_model(model, train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3278a4cb",
   "metadata": {},
   "source": [
    "## 7. Real-time Audio Processing\n",
    "\n",
    "Create functions for processing new audio samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922bc3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_audio(model, waveform, sample_rate):\n",
    "    \"\"\"Process and predict digit from audio input\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Preprocess audio\n",
    "        waveform = preprocess_audio(waveform, sample_rate)\n",
    "        \n",
    "        # Extract features\n",
    "        features = extract_features(waveform)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        features = torch.FloatTensor(features).unsqueeze(0).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get prediction\n",
    "        outputs = model(features)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        return predicted.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81505fd1",
   "metadata": {},
   "source": [
    "## 8. Microphone Integration\n",
    "\n",
    "Add live microphone input support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e57dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "\n",
    "def record_audio(duration=1, sample_rate=8000):\n",
    "    \"\"\"Record audio from microphone\"\"\"\n",
    "    print(\"Recording...\")\n",
    "    audio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1)\n",
    "    sd.wait()\n",
    "    return audio.flatten()\n",
    "\n",
    "def live_prediction():\n",
    "    \"\"\"Make real-time predictions from microphone input\"\"\"\n",
    "    while True:\n",
    "        input(\"Press Enter to record a 1-second audio clip (or Ctrl+C to exit)...\")\n",
    "        waveform = record_audio()\n",
    "        prediction = predict_audio(model, waveform, 8000)\n",
    "        print(f\"\\nPredicted digit: {prediction}\\n\")\n",
    "\n",
    "# Uncomment to test live predictions\n",
    "# live_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f058bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages if missing\n",
    "!python -m pip install sounddevice datasets librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80e4621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction on a sample\n",
    "test_sample = dataset['train'][10]\n",
    "waveform = test_sample['audio']['array']\n",
    "sample_rate = test_sample['audio']['sampling_rate']\n",
    "predicted_digit = predict_audio(model, waveform, sample_rate)\n",
    "print(f\"Predicted digit: {predicted_digit}, True label: {test_sample['label']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
